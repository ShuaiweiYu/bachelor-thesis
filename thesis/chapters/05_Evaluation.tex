\chapter{Evaluation}
\label{sec:evaluation}

\section{Text Input Analysis}
\begin{table}[]
\begin{center}
\caption{\centering Textual descriptions by source type}
\label{table:source_type}
\begin{tabular}{l|l|l}
         & Amount & Frequency \\ \hline
academic & 10     & 50.00\%   \\
industry & 6      & 30.00\%   \\
textbook & 4      & 20.00\%   \\ \hline
Total    & 20     & 100.00\% 
\end{tabular}
\end{center}
\end{table}

This work uses 20 textual descriptions in total as input for evaluation. Fifteen textual descriptions are taken from Friedrich's work \cite{t2m_1_main}, and another five descriptions are taken from other sources, guaranteeing a fair evaluation and benchmarking of the proposed system and ensuring that the designed system is not a tailored system. All the used textual descriptions can be found in appendix \ref{appendix:text}. The evaluation dataset consists of textual descriptions from various sources which enable the evaluation of the system's generalizability. Textual descriptions from Academic category are provided by universities or employees in the university, textual descriptions from Industry are collected from comapnies or their employees while textual descriptions from Textbook are gathered from modeling textbooks. The textual descriptions by source type and the proportion are given in table \ref{table:source_type}. The structure of the textual descriptions given in figure \ref{img:eva_info} varies from each other, with the most extended text having 30 sentences and the shortest having 4. The length of the sentence is always in range from 10-25 words. The generated BPMN diagram can be found in the GitHub repository of the author, the link is given in section \ref{sec:implementation}.

\begin{figure}[h]
    \centering
    \caption{Information of evaluation data set}
    \label{img:eva_info}
    \includegraphics[scale=0.4]{tum-resources/images/sentence_num.png}
    \hspace{1in}
     \includegraphics[scale=0.4]{tum-resources/images/avg_sentence_length.png}
\end{figure} 

Furthermore, the evaluation also leverages the PET dataset proposed in \cite{pet_dataset} to enhance the evaluation. The gold standard this work refers to is annotated with the help of the PET dataset using proposed \textit{ner-tags}, representing the relevant Actors, Activities, Specifications, and Gateways in the textual description. The annotated dataset allows the evaluation to identify the irrelevant process information and exploit the relationship between such information and the precision of generated models. A PET ratio for each PET annotated textual description is computed by $PET\_ratio = \frac{number\_of\_ner tags}{number\_of\_tokens}$. A high PET ratio indicates that the textual description contains high information density.

\begin{figure}[h]
    \centering
    \caption{PET token ratio}
    \includegraphics[width=0.5\textwidth]{tum-resources/images/PET_ratio.png}
\end{figure}

\section{Comparison of Occurrence}
We will use the BPMN diagram generated by work \cite{t2m_1_main} and the correspondent diagram modeled by a human modeler for benchmarking to evaluate the proposed approach. To quantify the evaluation, we will manually create a matching relationship between the models generated by our approach and the models generated in \cite{t2m_1_main}. The human modeler tends to model activities in an abstract and compact manner \cite{t2m_1_main}, while our system generates the activities with more extended information. The proposed manual matching allows the evaluation to ignore the minor differences in the activity names. The evaluation also allows this work to identify the information that the proposed system fails to extract and the activities or gateways that are misplaced. 

\begin{itemize}
    \item Number of Activities $A$
    \item Number of lanes $S$
    \item Number of parallel gateways $G_p$
    \item Number of exclusive gateways $G_e$
\end{itemize}

Some indicators are required to perform the evaluation where \cite{t2m_1_successor} provides us with an initial starting point. We will measure the number of activities in our model and Friedrich's model \cite{t2m_1_main} that matches with the model generated by human modelers. The elements identified by our approach are denoted using $our\_approach$, $baseline$ refers to the current state-of-art approach given by \cite{t2m_1}, while $gold\_standard$ stands for the diagram generated by human modelers. As our system might generate more activities than the model generated by humans, the matching allows us to match multiple activities from our output with a single activity in the target dataset. The successfully matched activities are denoted as $A^{our\_approach}_{matched}$. On the contrary, the activities that occurred in the target set but are not captured by our system are recorded using $A^{our\_approach}_{missed}$. Finally, the activities only captured in the proposed system but not in the target set are considered irrelevant information and thus denoted as $A^{our\_approach}_{irrelevant}$. The evaluation will also capture the number of lanes identified by our approach $S^{our\_approach}$ to evaluate whether the proposed system is capable of identifying the actual actor given in the gold standard $S^{gold\_standard}$ who performs the activities. By comparing the number of activities, lanes, gateways, the evaluation process gains a first insight into the similarities and differences between our system, the system designed by Friedrich, and the human modeler.  

\begin{table}[]
\caption{\centering Result of applying the evaluation metrics}
\label{table:eva_01}
\begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\rotatebox{90}{$ID$} &
  \multicolumn{1}{l|}{\rotatebox{90}{$A^{our\_approach}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$A^{baseline}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$A^{gold\_standard}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$A^{our\_approach}_{matched}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$A^{our\_approach}_{missed}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$A^{our\_approach}_{irrelevant}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$S^{our\_approach}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$S^{gold\_standard}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$G_p^{our\_approach}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$G_p^{baseline}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$G_p^{gold\_standard}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$G_e^{our\_approach}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$G_e^{baseline}$}} &
  \multicolumn{1}{l|}{\rotatebox{90}{$G_e^{gold\_standard}$}} \\ \hline
1-1 & 16 & 18 & 9  & 10 & 1 & 6  & 5 & 3 & 2 & 2 & 2 & 6  & 6 & 4 \\ \hline
1-2 & 13 & 10 & 8  & 10 & 0 & 2  & 2 & 2 & 0 & 0 & 0 & 4  & 3 & 3 \\ \hline
1-3 & 16 & 21 & 12 & 11 & 1 & 5  & 3 & 4 & 0 & 4 & 3 & 2  & 3 & 4 \\ \hline
3-1 & 11 & 10 & 6  & 8  & 1 & 3  & 3 & 3 & 0 & 0 & 0 & 0  & 0 & 0 \\ \hline
3-5 & 17 & 15 & 13 & 15 & 0 & 2  & 4 & 4 & 2 & 2 & 0 & 2  & 2 & 0 \\ \hline
3-6 & 10 & 10 & 6  & 5  & 0 & 5  & 1 & 1 & 0 & 0 & 0 & 4  & 7 & 3 \\ \hline
3-8 & 13 & 11 & 12 & 10 & 0 & 3  & 3 & 4 & 0 & 0 & 0 & 2  & 2 & 2 \\ \hline
5-1 & 7  & 8  & 6  & 4  & 0 & 3  & 1 & 1 & 0 & 0 & 0 & 2  & 3 & 1 \\ \hline
5-2 & 9  & 7  & 5  & 8  & 0 & 1  & 4 & 3 & 0 & 0 & 0 & 4  & 1 & 1 \\ \hline
5-3 & 23 & 20 & 10 & 7  & 1 & 16 & 6 & 3 & 0 & 0 & 0 & 6  & 5 & 1 \\ \hline
8-2 & 12 & 10 & 8  & 9  & 0 & 3  & 1 & 1 & 0 & 0 & 0 & 2  & 1 & 1 \\ \hline
6-3 & 14 & 18 & 6  & 5  & 1 & 9  & 1 & 3 & 0 & 0 & 0 & 2  & 1 & 2 \\ \hline
6-4 & 21 & 16 & 14 & 11 & 2 & 10 & 5 & 5 & 0 & 0 & 0 & 10 & 8 & 8 \\ \hline
6-1 & 32 & 35 & 17 & 15 & 2 & 17 & 5 & 6 & 0 & 0 & 0 & 6  & 2 & 6 \\ \hline
2-2 & 48 & 46 & 24 & 23 & 3 & 25 & 4 & 4 & 0 & 0 & 2 & 14 & 9 & 6 \\ \hline
0-1 & 28 & /  & 8  & 7  & 4 & 5  & 2 & 4 & 0 & / & 0 & 4  & / & 5 \\ \hline
0-2 & 13 & /  & 8  & 10 & 0 & 3  & 2 & 3 & 0 & / & 0 & 4  & / & 4 \\ \hline
0-3 & 15 & /  & 7  & 8  & 1 & 7  & 4 & 2 & 0 & / & 0 & 4  & / & 4 \\ \hline
0-4 & 18 & /  & 14 & 13 & 3 & 5  & 3 & 3 & 0 & / & 2 & 0  & / & 2 \\ \hline
0-5 & 14 & /  & 12 & 9  & 1 & 5  & 1 & 1 & 0 & / & 0 & 6  & / & 3 \\ \hline
\end{tabular}
\end{table}

The result of the evaluation of the proposed system is given in the table \ref{table:eva_01} after applying the metrics. The target set's textual descriptions and the corresponding BPMN diagrams are marked with an attribute \textit{ID}. During the evaluation, the number of activities and Gateways generated by our system, by Friedrich's system and by human modelers are compared. The deviation rate between Activities generated by different sources can be calculated as: $R(M_{compare}, M_{target}) = \Sigma\frac{|A_{compare} - A_{target}|}{A_{target}}$. Therefore, The difference between our system and Friedrich's system is given as $R(M, M_{Friedrich}) = 16.53\%$. Friedrich's system generates a total of 61 gateways, and our system generates a total of 58 gateways, while the total gateways generated in the first 15 textual descriptions is 42 (the last five textual descriptions are not from Friedrich's work). The average number of missed activity can be calculated through $\overline{missed\ activity} = \frac{\Sigma^n A_{missed}}{n}$, which leads to a result of 1.05.

We can define the \textit{share of matched activities} as $\frac{A_{matched}}{A}$, indicating that the number of activities generated by our system that matches the the activities generated by human modelers. The higher this quotient is, the less irrelevant activities our model contains. Figure \ref{img:eva_li01} reveals the relationship between the share of matched activities with the sentence length. The correlation between number of sentences and share of matched activities is calculated as -0.42 and between average sentence length and share of matched activities is calculated as -0.31, which both indicate a weak correlation between these variable. Yet the correlation between PET token ratio and share of matched activities reached 0.52, which suggests a medium correlation relationship. 

\begin{figure}[h]
    \centering
    \caption{Lineal regression regarding sentence length}
    \label{img:eva_li01}
    \includegraphics[scale=0.4]{tum-resources/images/eva01.png}
    \hspace{1in}
     \includegraphics[scale=0.4]{tum-resources/images/eva02.png}
\end{figure}

\begin{figure}[h]
    \centering
    \caption{Lineal regression regarding PET token ratio}
    \label{img:eva_li02}
    \includegraphics[width=0.5\textwidth]{tum-resources/images/eva03.png}
\end{figure}
\subsection{Comparison between the Baseline System and the Proposed Approach}
\subsection{Comparison between the gold standard and the Proposed Approach}
\subsection{Improvement of LLM}
This work leverages the Large Language Model to perform the adjustment and correction on the generated output by using the LLM's outstanding ability in semantic understanding, which is vital in BPMN diagram modeling. The first task LLM should perform is the removal of process irrelevant information. From table \ref{table:eva_01}, it can be concluded that our system generates a total number of 128 redundant activities (not including text 1-2 and 1-3), while the result adjusted using LLM only generates 82 redundant activities without removing the valid activities.

The evaluation also shows that the LLM can assist with the anaphora resolution and the identification of business process participants. The table \ref{table:eva_LLM_actor} shows that the proposed system can reach an average precision rate of 65\% and a recall rate of 69\%. After the output is further processed by the GPT-4, the precision and recall rates are raised. However, the LLM can not achieve a performance increase every time on the individual level. The performance might drop, for example, in text 5-3, where LLM extracted more actors than needed, which leads to a decrease in the precision rate. 

\begin{table}[]
\caption{\centering Evaluation of lanes before and after applying the LLM}
\label{table:eva_LLM_actor}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{ID} & $S^{work}_{precision}$      & $S^{work}_{recal}$           & $F_1-score^{work}$        & $S^{LLM}_{precision}$ & $S^{LLM}_{recal}$  &   $F_1-score^{LLM}$    \\ \hline
1-1          & 0.60             & 1.00             & 0.75             & 0.75             & 1.00             & 0.86             \\ \hline
1-2          & 1.00             & 1.00             & 1.00             & \textbackslash{} & \textbackslash{} & \textbackslash{} \\ \hline
1-3          & 0.80             & 1.00             & 0.89             & \textbackslash{} & \textbackslash{} & \textbackslash{} \\ \hline
3-1          & 0.33             & 0.33             & 0.33             & 0.33             & 0.33             & 0.33             \\ \hline
3-5          & 0.75             & 0.75             & 0.75             & 0.75             & 0.75             & 0.75             \\ \hline
3-6          & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} \\ \hline
3-8          & 1.00             & 0.75             & 0.86             & 1.00             & 0.75             & 0.86             \\ \hline
5-1          & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} \\ \hline
5-2          & 0.75             & 1.00             & 0.86             & 0.75             & 1.00             & 0.86             \\ \hline
5-3          & 0.50             & 1.00             & 0.67             & 0.43             & 1.00             & 0.60             \\ \hline
8-2          & 0.50             & 0.50             & 0.50             & 1.00             & 1.00             & 1.00             \\ \hline
6-3          & 0.33             & 0.33             & 0.33             & 0.75             & 1.00             & 0.86             \\ \hline
6-4          & 0.80             & 0.80             & 0.80             & 0.80             & 0.80             & 0.80             \\ \hline
6-1          & 0.80             & 0.67             & 0.73             & 0.63             & 0.83             & 0.71             \\ \hline
2-2          & 0.50             & 0.50             & 0.50             & 0.80             & 1.00             & 0.89             \\ \hline
0-1          & 1.00             & 0.50             & 0.67             & 0.75             & 0.75             & 0.75             \\ \hline
0-2          & 0.50             & 0.33             & 0.40             & 0.20             & 0.33             & 0.25             \\ \hline
0-3          & 0.50             & 1.00             & 0.67             & 0.50             & 1.00             & 0.67             \\ \hline
0-4          & 0.33             & 0.33             & 0.33             & 0.67             & 0.67             & 0.67             \\ \hline
0-5          & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} & \textbackslash{} \\ \hline
\textbf{avg} & \textbf{0.65}    & \textbf{0.69}    & \textbf{0.65}    & \textbf{0.67}    & \textbf{0.81}    & \textbf{0.72}    \\ \hline
\end{tabular}
\end{table}













\section{Comparison of Order}
Furthermore, the evaluation also focuses on measuring the accuracy and correctness of the sequence flow. Friedrich's work leverages evaluation metrics \textit{Graph Edit Distance} introduced in \cite{eva_01} to measure the similarity between generated and human-modeled diagrams. However, the similarity functions are majorly focused on the syntactic or semantic aspects of the BPMN model, and little attention is given to evaluating the correctness of the order in the model. Therefore, this work proposes a way to evaluate this aspect by using \textit{Log-based ordering relations} introduced in \cite{eva_02}. 

Consider $L$ to be an event log and $a$, $b$ are elements belonging to $L$. 

\begin{definition}[directly follows]
$a >_L b$ if and only if there is a trace $\sigma =<t_1, t_2, t_3,...,t_n>$and $i \in \{1,...,n-1\}$ such that $\sigma \in L$ and $t_i = a$ and $t_i+1 = b$ \cite{eva_02}
\end{definition}

\begin{definition}[causality]
$a \rightarrow_L b$ if and only if $a >_L b$ and not $b >_L a$ \cite{eva_02}
\end{definition}
Consider $L = [<a,b,c,d>, <a,c,b,d>]$ as an event log, a causal ordering relation in this case can be $L = \{(a,b), (c,d), (a,c), (b,d)\}$, indicating that $a$ as a start event is followed by $b$ and $c$, who are in a parallel relationship and finally join together into the end event $d$. The notation $\rightarrow_L$ contains every activity pair that is in a "causality" relation. The causality property indicates that $a$ and $b$ are in sequence, which can be used to construct the corresponding causal log-based ordering relation \cite{eva_02}. This work will use the BPMN diagram modeled by humans (gold standard) as a reference and construct such an ordering relation. Each activity in the gold standard diagram will be marked with an activity label. In the next step, we will search for the activities that has the same label within the diagram generated by our system and then construct an ordering relations, which will be used for comparison with the gold standard. The pairs within two ordering relations will be compared during the evaluation process. Only if the pairs are identical they are considered as a match. The precision rate, recall rate and $F_1$ score \cite{t2m_5} between the baseline system with the gold standard and our work with the gold standard are calculated and given in table \ref{table:eva_seq_01} and \ref{table:eva_seq_02}. The result shows that our approach achieves a total $F_1$ score of 0.68 using textual descriptions taken from \cite{t2m_1_main}, which is slightly improved compared to the result of 0.59 in Friedrich's work. Comparing the result from table \ref{table:eva_01}, the result also suggests that the amount of redundant information has an impact on the precision of the recognition of sequence flow. However, the factor that has the most significant effect is the jumps and implications of gateways in the textual description because correctly mapping this requires the semantic understanding ability, which our rule-based approach lacks. 

\begin{table}[]
\caption{\centering Evaluation of activity sequence's correctness for textual descriptions taken from paper \cite{t2m_1_main}}
\label{table:eva_seq_01}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\textbf{ID} & $A^{baseline}_{precision}$      & $A^{baseline}_{recal}$           & $F_1-score^{baseline}$        & $A^{our\_approach}_{precision}$ & $A^{our\_approach}_{recal}$  &   $F_1-score^{our\_approach}$    \\ \hline
1-1 & 0.71             & 0.67             & 0.69             & 0.75 & 0.80 & 0.77 \\ \hline
1-2 & 0.61             & 0.53             & 0.57             & 0.75 & 0.60 & 0.67 \\ \hline
1-3 & 0.80             & 0.61             & 0.69             & 0.80 & 0.61 & 0.69 \\ \hline
3-1 & 0.80             & 0.66             & 0.72             & 1.00 & 1.00 & 1.00 \\ \hline
3-5 & 0.54             & 0.50             & 0.52             & 0.69 & 0.79 & 0.74 \\ \hline
3-6 & 0.80             & 0.57             & 0.67             & 0.67 & 0.57 & 0.62 \\ \hline
3-8 & 0.86             & 0.67             & 0.75             & 0.89 & 0.89 & 0.89 \\ \hline
5-1 & 0.60             & 0.43             & 0.50             & 0.60 & 0.43 & 0.50 \\ \hline
5-2 & 1.00             & 1.00             & 1.00             & 1.00 & 1.00 & 1.00 \\ \hline
5-3 & 0.50             & 0.30             & 0.38             & 0.90 & 0.90 & 0.90 \\ \hline
8-2 & 0.29             & 0.20             & 0.24             & 0.29 & 0.20 & 0.24 \\ \hline
6-3 & 0.80             & 0.44             & 0.57             & 0.80 & 0.44 & 0.57 \\ \hline
6-4 & 0.70             & 0.53             & 0.60             & 0.45 & 0.38 & 0.41 \\ \hline
6-1 & 0.71             & 0.29             & 0.41             & 0.73 & 0.47 & 0.57 \\ \hline
2-2 & 0.64             & 0.39             & 0.48             & 0.69 & 0.48 & 0.57 \\ \hline
\textbf{avg} & \textbf{0.69} & \textbf{0.52} & \textbf{0.59} & \textbf{0.73}     & \textbf{0.64} & \textbf{0.68}  \\ \hline
\end{tabular}
\end{table}


\begin{table}[]
\caption{\centering Evaluation of activity sequence's correctness for textual descriptions taken from other sources}
\label{table:eva_seq_02}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{ID}  & $A^{our\_approach}_{precision}$ & $A^{our\_approach}_{recal}$  &   $F_1-score^{our\_approach}$    \\ \hline
0-1 & 0.88 & 0.58 & 0.70 \\ \hline
0-2 & 0.58 & 0.58 & 0.58 \\ \hline
0-3 & 0.44 & 0.44 & 0.44 \\ \hline
0-4 & 0.67 & 0.50 & 0.57 \\ \hline
0-5 & 0.42 & 0.25 & 0.31 \\ \hline
\textbf{avg} & \textbf{0.60} & \textbf{0.47} & \textbf{0.52}  \\ \hline
\end{tabular}
\end{table}
\subsection{Comparison between the Baseline System and the Proposed Approach}
\subsection{Comparison between the gold standard and the Proposed Approach}
\subsection{Improvement of LLM}
















































































\section{Comparison to Friedrich's approach}
This work implements some algorithms given in Friedrich's approach \cite{t2m_1_main}, yet many modifications are made to increase the performance and functionality of the BPMN auto-generation. The evaluation shows work's several improvements compared to the Friedrich's approach.

The proposed approach generates a total number of 262 Activities, 4 Parallel Gateways and 66 Exclusive Gateways while Friedrich's approach generates in total 255 Activities, 8 Parallel Gateways and 53 Exclusive Gateways. The number of generated BPMN elements of two works are in close alignment. However, some further modifications are made with the help of the more state-of-art machine learning technologies like \textit{spacy} and adjustment of the algorithms.

One modification is made regarding the generation of the end event. Suppose an end event occurs after a gateway (business process ends). In that case, Friedrich's approach will not introduce this end event after the gateways but for every branch inside the gateway as demonstrated in figure \ref{img:end_event_gen:left} , which is not desired and might lead to confusion and redundancy \cite{BPMN_tech_report}. Our approach adjusts the data structure of the gateways and the strategy of end activity generation to avoid such a situation and only generate end events after the gateways if the business process ends which is demonstrated in \ref{img:end_event_gen:right}. 


\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tum-resources/images/eva_F_gate.png}
         \caption{result from the baseline system}
         \label{img:end_event_gen:left}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tum-resources/images/eva_us_gate.png}
         \caption{result from our system}
         \label{img:end_event_gen:right}
     \end{subfigure}
     \hfill
     \caption{Different end event generation strategy}
     \label{img:end_event_gen}
\end{figure}

Our work also enhanced the ability to perform semantic analysis by examining the activities that indicate an end event within a gateway, which is not implemented by Friedrich's approach. An example of such recognition is given in figure \ref{img:end_event_rec}, our model detects that the claim is rejected, which indicates that the process will end, shown in figure \ref{img:end_event_rec:right}. Therefore, an end event is added after this activity. The introduction of the new marker detection process, as given in figure \ref{img:case}, also allows our system to add the activities mentioned later using "in the latter/former case" into the proper gateways, as displayed in figure \ref{img:case:right}, which is not captured by the baseline system as shown in figure \ref{img:case:left}. This increases the accuracy of the generated BPMN diagram. 

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tum-resources/images/eva_end_detect_F.png}
         \caption{result from the baseline system}
         \label{img:end_event_rec:left}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tum-resources/images/eva_end_detect_us.png}
         \caption{result from our system}
         \label{img:end_event_rec:right}
     \end{subfigure}
     \hfill
     \caption{Semantic end events detection}
     \label{img:end_event_rec}
\end{figure}

\begin{figure}
     \centering
     \begin{subfigure}[b]{0.4\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tum-resources/images/eva_case_F.png}
         \caption{result from the baseline system}
         \label{img:case:left}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.5\textwidth}
         \centering
         \includegraphics[width=\textwidth]{tum-resources/images/eva_case_us.png}
         \caption{result from our system}
         \label{img:case:right}
     \end{subfigure}
     \hfill
     \caption{Gateway elements complement}
     \label{img:case}
\end{figure}

Furthermore, the activities extracted by our system are finer than Friedrich's approach due to the divide-and-conquer design principle leveraged, which ensures finer information granularity and atomicity. This guarantees that an activity name within an activity will be a manageable length and thus becomes more comprehensive. Using a different intermediate storage also allows our approach to generate more understandable activity names, whereas some activity names generated by Friedrich's approach are rarely understandable.

\section{Comparison to Human-Generated Models}
The comparison between the BPMN diagram generated by our approach and the diagram generated by human modelers given in figure \ref{img:eva_comp_human} offers a qualitative evaluation of the proposed approach. Our approach generates 63\% more Activities, 40\% more Exclusive Gateways, and 45\% less Parallel Gateways than the human modelers. The generation of lanes functions exceptionally, the total amount generated matches the result of the human modeler, while the number of lanes generated for each diagram deviates little from the human's result. One drawback of our approach is that our system can only generate pools but not lanes, which is commonly used in the BPMN diagram generated by humans. 

\begin{figure}[h]
    \centering
    \caption{Comparison between the BPMN diagram generated by our system and by human modelers}
    \label{img:eva_comp_human}
    \includegraphics[scale=0.4]{tum-resources/images/eva_human_01.png}
    \hspace{1in}
    \includegraphics[scale=0.4]{tum-resources/images/eva_human_02.png}
    \hspace{1in}
    \includegraphics[scale=0.4]{tum-resources/images/eva_human_03.png}
    \hspace{1in}
    \includegraphics[scale=0.4]{tum-resources/images/eva_human_04.png}
\end{figure}

This work tears down the sentences into many clauses so that the algorithms can capture the details of each clause to achieve maximal information retrieval. The result turns out to be positive, where there is only a small amount of missed activity for each textual description. However, this design idea also leads to a high redundancy rate in the generated BPMN diagram. The reason behind this is the problem of relevance mentioned in the section \ref{sec:solution:relevance}. As our approach follows the descriptive order of the textual input, such irrelevant information will interfere with the generated results. The rate of irrelevant information a diagram contains is correlated with the number of sentences and the average sentence length. The longer the textual description is, the higher the possibility that the textual description contains information that offers examples and meta information. The average length of a sentence could also slightly influence the result because longer sentences have complex structures and thus make clause decomposition difficult. Our approach's generated activity names are also longer than those generated by humans, as human models are customed to model the activity names in an abstract manner \cite{t2m_1_main}. 

