\chapter{Evaluation}

Evaluation is an essential aspect of artifact construction. It allows us to analyze the quality of the proposed artifact and based on which we can decide whether further modification and improvement should be made. The general goal of evaluation is whether the proposed approach satisfies the need of auto-generating BPMN models from textaul documents in actual practice and how well the proposed approach performs in extracting the BPMN model manually. The evaluation can be divided into to part: qualitative and quantitative evaluation. 

%qualitative -> what does our model is good at and not
In the qualitative evaluation, the work plana to explore the pros and cons of the proposed approach. During this phase, the basic components of the approach will be analyzed to find out their strengths and limitations in handling different kinds of text input. For example, we could use \textit{Spacy} or \textit{Stanford Parser} to perform part-of-speech tagging, and a qualitative evaluation is also needed to see which one is more suitable for the approach. Furthermore, information extraction stage will be examined and analyzed whether the proposed information strategies are logical. Through such inspections, this work could probably gain new insights and are able to optimize the proposed approach. Moreover, we will also consult the experts from the BPMN modeling field for their suggestions on the generated process model to ensure our model complies with the BPMN modeling rules. The objective of the qualitative evaluation is to provide us with the functional information of the approach so that corresponding improvements can be made or relevant such tasks can be left as a potential challenge for future work.

%quantitative -> model acuracy/benchmarking with the previous work
As for quantitative evaluation, the work want to compare how well the automatically extracted BPMN models approximate the models created manually. In order to perform such accuracy checking, this work will, in the first step, collect text documents and other types of documents from different sources and manually model the BPMN model regarding the BPMN modeling rules. Then, the same documents will be used as input for our approach and let the program automatically generates BPMN models. In the next step, the similarity of these two models will be compared by using the metric of \textit{Graph Edit Distance} as proposed in \cite{t2m_1}, which captures the similarity of two graphs and quantitatively measures them.

Another task this work should perform in the quantitative evaluation is benchmarking the approach with the previous works. The purpose of this task is to examine whether the proposed model offers a novel breakthrough. To achieve this, the documents used in the previous works will be gathered and this work will try to the documents generate the BPMN models. Then calculate the similarity will be calculated and this work will benchmark it with the accuracy rate given in the previous works. Furthermore, the running time for the model generation is also desired. The work would like to find out whether the newly-emerged technique applied offers a better running time compared to the techniques invented years ago. 

 Suppose our prototype generates BPMN models that are highly similar to the models created by experts. In that case, it seems fair to regard our prototype as an effective solution for BPMN model generation and can assist with the time-consuming modeling tasks. However, if the generated model's accuracy is low, the work must head back to the prototype design steps and improve business activity recognition and model generation. If the running time of our approach is too long, some works have to be done to reduce the time complexity of the proposed approach.