\chapter{Evaluation}

Evaluation is an essential aspect of artifact construction. It allows us to analyze the quality of our artifact and based on which we can decide whether further modification and improvement should be made. The general goal of evaluation is whether the proposed approach satisfies the need of auto-generating BPMN models from regulatory documents in actual practice and how well the proposed approach performs in extracting the BPMN model manually. The evaluation can be divided into to part: qualitative and quantitative evaluation. 

%qualitative -> what does our model is good at and not
In the qualitative evaluation, we plan to explore the pros and cons of our approach. During this phase, we will analyze the basic components of our approach to find out their strengths and limitations in handling different kinds of text input. For example, we could use \textit{Spacy} or \textit{Stanford Parser} to perform part-of-speech tagging, and we should run a qualitative evaluation to see which one is more suitable for our approach. Furthermore, we will also look into our information extraction stage and analyze whether the proposed information strategies are logical. Through such inspections, we could probably gain new insights and are able to optimize our approach. Moreover, we will also consult the experts from the BPMN modeling field for their suggestions on the generated process model to ensure our model complies with the BPMN modeling rules. The objective of the qualitative evaluation is to provide us with the functional information of our approach so that we can make corresponding improvements or leave such tasks as a potential challenge for future work.

%quantitative -> model acuracy/benchmarking with the previous work
As for quantitative evaluation, we want to compare how well the automatically extracted BPMN models approximate the models created manually. In order to perform such accuracy checking, we will, in the first step, collect regulatory documents and other types of documents from different sources and manually model the BPMN model regarding the BPMN modeling rules. Then, we will use the same documents as input for our approach and let the program automatically generates BPMN models. In the next step, we will compare the similarity of these two models by using the metric of \textit{Graph Edit Distance} as proposed in \cite{t2m_1}, which captures the similarity of two graphs and quantitatively measures them.

Another task we should perform in the quantitative evaluation is benchmarking our approach with the previous works. The purpose of this task is to examine whether our model offers a novel breakthrough. To achieve this, we will gather the documents used in the previous works and try to generate the BPMN models. We will then calculate the similarity and benchmark it with the accuracy rate given in the previous works. Furthermore, we also want to measure the running time for the model generation. We would like to find out whether the newly-emerged technique we apply offers a better running time compared to the techniques invented years ago. 

 Suppose our prototype generates BPMN models that are highly similar to the models created by experts. In that case, it seems fair to regard our prototype as an effective solution for BPMN model generation and can assist with the time-consuming modeling tasks. However, if the generated model's accuracy is low, we must head back to the prototype design steps and improve business activity recognition and model generation. If the running time of our approach is too long, we have to work to reduce the time complexity of our approach.